{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resave_incs(path, dest_path):\n",
    "    pth = Path(path)\n",
    "    for child in pth.iterdir():\n",
    "        # incidents data reading\n",
    "        incdnt_file_name = child.name\n",
    "        inc_header = ['IncidentID', 'CC_Code', 'Incident_No', 'Timestamp', 'Description', 'Location', 'Area', 'Zoom_Map', 'TBxy', 'Latitude', 'Longitude', 'District', 'CountryFIPS_ID', 'CityFIPS_ID', 'Freeway', 'Freeway_direction', 'State_postmile', 'Absolute_postmile', 'Severity', 'Duration']\n",
    "        data_inc_d07 = pd.read_csv(path+'/'+incdnt_file_name, sep=',', names=inc_header, parse_dates=[3])\n",
    "        data_inc_d07 = (data_inc_d07.dropna(subset=['District']))[data_inc_d07.columns[:-4]]\n",
    "        data_inc_d07 = data_inc_d07.astype(dtype={'District':int})\n",
    "        # result: incidents in district under analysis\n",
    "        data_inc_d07 = data_inc_d07[data_inc_d07['District']==7].reset_index(drop=True)\n",
    "        data_inc_d07.to_csv(dest_path+'/'+child.name[:-4]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resave_incs('../data/PeMS/Incidents/work_folder/Months/inc/inc/raw/', '../data/PeMS/Incidents/work_folder/Months/inc/inc/light/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_path = '../data/PeMS/Incidents/test_env/inc/inc/'\n",
    "dest_path = '../data/PeMS/Incidents/test_env/inc/light/'\n",
    "pth = Path(raw_path)\n",
    "for child in pth.iterdir():\n",
    "    # incidents data reading\n",
    "    incdnt_file_name = child.name\n",
    "    inc_header = ['IncidentID', 'CC_Code', 'Incident_No', 'Timestamp', 'Description', 'Location', 'Area', 'Zoom_Map', 'TBxy', 'Latitude', 'Longitude', 'District', 'CountryFIPS_ID', 'CityFIPS_ID', 'Freeway', 'Freeway_direction', 'State_postmile', 'Absolute_postmile', 'Severity', 'Duration']\n",
    "    data_inc_d07 = pd.read_csv(raw_path+'/'+incdnt_file_name, sep=',', names=inc_header, parse_dates=[3])\n",
    "    data_inc_d07 = (data_inc_d07.dropna(subset=['District']))[data_inc_d07.columns[:-4]]\n",
    "    data_inc_d07 = data_inc_d07.astype(dtype={'District':int})\n",
    "    # result: incidents in district under analysis\n",
    "    data_inc_d07 = data_inc_d07[data_inc_d07['District']==7].reset_index(drop=True)\n",
    "    data_inc_d07.to_csv(dest_path+'/'+child.name[:-4]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# incidents data reading\n",
    "incdnt_file_name = '../data/PeMS/Incidents/raw/all_text_chp_incident_day_2017_03_06.txt.gz'\n",
    "inc_header = ['IncidentID', 'CC_Code', 'Incident_No', 'Timestamp', 'Description', 'Location', 'Area', 'Zoom_Map', 'TBxy', 'Latitude', 'Longitude', 'District', 'CountryFIPS_ID', 'CityFIPS_ID', 'Freeway', 'Freeway_direction', 'State_postmile', 'Absolute_postmile', 'Severity', 'Duration', 'Incident_ID', 'Detail_ID', 'Timestamp', 'description']\n",
    "data_inc_d07 = pd.read_csv(incdnt_file_name, sep=',', names=inc_header, parse_dates=[3])\n",
    "data_inc_d07 = (data_inc_d07.dropna(subset=['District']))[data_inc_d07.columns[:-4]]\n",
    "data_inc_d07 = data_inc_d07.astype(dtype={'District':int})\n",
    "# result: incidents in district under analysis\n",
    "data_inc_d07 = data_inc_d07[data_inc_d07['District']==7].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_inc_d07.to_csv('../data/PeMS/Incidents/light/all_text_chp_incident_day_2017_03_06.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incidents details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_filter_desc(path, dest_path):\n",
    "    pth = Path(path)\n",
    "    det_header = ['ID', 'DetID', 'Timestamp', 'Desc']\n",
    "    for child in pth.iterdir():\n",
    "        incdnt_det_file_name = path + '/' + child.name\n",
    "        det_month = pd.read_csv(incdnt_det_file_name, sep=',', names=det_header, parse_dates=[2])\n",
    "        det_month.dropna(axis=0, inplace=True)\n",
    "        det_month = det_month.astype({'ID': np.int, 'DetID': np.int})\n",
    "        det_month.to_csv(dest_path + '/' + child.name[:-4] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_filter_desc('../data/PeMS/Incidents/work_folder/det/raw/', '../data/PeMS/Incidents/work_folder/det/light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incdnt_det_file_name = '../data/PeMS/Incidents/test_env/det/orig/all_text_chp_incident_det_month_2017_01.txt'\n",
    "det_header = ['ID', 'DetID', 'Timestamp', 'Desc']\n",
    "data_inc_det_d07 = pd.read_csv(incdnt_det_file_name, sep=',', names=det_header, parse_dates=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_inc_det_d07.to_csv('../data/PeMS/Incidents/test_env/det/orig/all_text_chp_incident_det_month_2017_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d07_text_station_5min_2017_12_22.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/tslib.pyx\", line 2270, in pandas._libs.tslib.array_to_datetime\n",
      "  File \"pandas/_libs/src/datetime.pxd\", line 119, in datetime._string_to_dts\n",
      "ValueError: Error parsing datetime string \"12/22/2017 04:00:00\" at position 2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 3022, in converter\n",
      "    infer_datetime_format=infer_datetime_format\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\", line 380, in to_datetime\n",
      "    result = _convert_listlike(arg, box, format)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\", line 294, in _convert_listlike\n",
      "    require_iso8601=require_iso8601\n",
      "  File \"pandas/_libs/tslib.pyx\", line 2156, in pandas._libs.tslib.array_to_datetime\n",
      "  File \"pandas/_libs/tslib.pyx\", line 2292, in pandas._libs.tslib.array_to_datetime\n",
      "  File \"pandas/_libs/tslibs/parsing.pyx\", line 99, in pandas._libs.tslibs.parsing.parse_datetime_string\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 1168, in parse\n",
      "    return DEFAULTPARSER.parse(timestr, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 556, in parse\n",
      "    res, skipped_tokens = self._parse(timestr, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 675, in _parse\n",
      "    l = _timelex.split(timestr)         # Splits the timestr into tokens\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 192, in split\n",
      "    return list(cls(s))\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 180, in __next__\n",
      "    def __next__(self):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-fb1b3f7b0d2e>\", line 24, in <module>\n",
      "    data_srs_smoothed = pd.read_csv(raw_path+'/'+srs_file_name, sep=',', names=header_srs, parse_dates=[0])\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 709, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 455, in _read\n",
      "    data = parser.read(nrows)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1069, in read\n",
      "    ret = self._engine.read(nrows)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1916, in read\n",
      "    names, data = self._do_date_conversions(names, data)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1667, in _do_date_conversions\n",
      "    self.index_names, names, keep_date_col=self.keep_date_col)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 3075, in _process_date_conversion\n",
      "    data_dict[colspec] = converter(data_dict[colspec])\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\", line 3026, in converter\n",
      "    parsing.try_parse_dates(strs, dayfirst=dayfirst))\n",
      "  File \"pandas/_libs/tslibs/parsing.pyx\", line 419, in pandas._libs.tslibs.parsing.try_parse_dates\n",
      "  File \"pandas/_libs/tslibs/parsing.pyx\", line 411, in pandas._libs.tslibs.parsing.try_parse_dates.lambda\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 1168, in parse\n",
      "    return DEFAULTPARSER.parse(timestr, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 587, in parse\n",
      "    if (isinstance(tzinfos, collections.Callable) or\n",
      "  File \"/opt/anaconda3/lib/python3.6/abc.py\", line 178, in __instancecheck__\n",
      "    def __instancecheck__(cls, instance):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/lib/python3.6/inspect.py\", line 730, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/py/_apipkg.py\", line 171, in __getattribute__\n",
      "    return getattr(getmod(), name)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/py/_apipkg.py\", line 155, in getmod\n",
      "    x = importobj(modpath, None)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/py/_apipkg.py\", line 48, in importobj\n",
      "    module = __import__(modpath, None, None, ['__doc__'])\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pytest.py\", line 27, in <module>\n",
      "    _preloadplugins() # to populate pytest.* namespace so help(pytest) works\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/_pytest/config.py\", line 109, in _preloadplugins\n",
      "    _preinit.append(get_config())\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/_pytest/config.py\", line 118, in get_config\n",
      "    pluginmanager.import_plugin(spec)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/_pytest/config.py\", line 426, in import_plugin\n",
      "    __import__(importspec)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/_pytest/python.py\", line 17, in <module>\n",
      "    from _pytest import fixtures\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/_pytest/fixtures.py\", line 3, in <module>\n",
      "    from py._code.code import FormattedExcinfo\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 764, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 833, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_path = '../data/PeMS/Series/raw'\n",
    "raw_path = '/home/frolovconst/Dvlpt/prj/Python/tc_w_ml/data/PeMS/Series/Batch/2017/Dec/'\n",
    "dest_path = '../data/PeMS/Series/light'\n",
    "dest_path = '/home/frolovconst/Dvlpt/prj/Python/tc_w_ml/data/PeMS/Series/Batch/light/Dec'\n",
    "pth = Path(raw_path)\n",
    "st_blacklist = np.array([])\n",
    "header_srs = ['Timestamp', 'Station','District', 'Freeway', 'Direction of Travel', 'Lane Type', 'Station Length',\n",
    "          'Samples', '% Observed', 'Total Flow', 'Avg Occupancy', 'Avg Speed', \n",
    "          'Lane 1 Samples', 'Lane 1 Flow', 'Lane 1 Avg Occ', 'Lane 1 Avg Speed', 'Lane 1 Observed',\n",
    "          'Lane 2 Samples', 'Lane 2 Flow', 'Lane 2 Avg Occ', 'Lane 2 Avg Speed', 'Lane 2 Observed',\n",
    "          'Lane 3 Samples', 'Lane 3 Flow', 'Lane 3 Avg Occ', 'Lane 3 Avg Speed', 'Lane 3 Observed',\n",
    "          'Lane 4 Samples', 'Lane 4 Flow', 'Lane 4 Avg Occ', 'Lane 4 Avg Speed', 'Lane 4 Observed',\n",
    "          'Lane 5 Samples', 'Lane 5 Flow', 'Lane 5 Avg Occ', 'Lane 5 Avg Speed', 'Lane 5 Observed',\n",
    "          'Lane 6 Samples', 'Lane 6 Flow', 'Lane 6 Avg Occ', 'Lane 6 Avg Speed', 'Lane 6 Observed',\n",
    "          'Lane 7 Samples', 'Lane 7 Flow', 'Lane 7 Avg Occ', 'Lane 7 Avg Speed', 'Lane 7 Observed',\n",
    "          'Lane 8 Samples', 'Lane 8 Flow', 'Lane 8 Avg Occ', 'Lane 8 Avg Speed', 'Lane 8 Observed']\n",
    "for child in pth.iterdir():\n",
    "    # incidents data reading\n",
    "    srs_file_name = child.name\n",
    "#     if srs_file_name[0] !='p':\n",
    "#         continue\n",
    "    print(srs_file_name)\n",
    "\n",
    "    data_srs_smoothed = pd.read_csv(raw_path+'/'+srs_file_name, sep=',', names=header_srs, parse_dates=[0])\n",
    "\n",
    "    for col in data_srs_smoothed.columns[-40:]:\n",
    "        del data_srs_smoothed[col]\n",
    "    data_srs_smoothed.dropna(subset=['Avg Speed'], inplace=True)\n",
    "    data_srs_smoothed.to_csv(dest_path+'/'+child.name[:-4]+'.csv', index=False)\n",
    "    #smoothing\n",
    "#     data_srs_smoothed = data_srs_smoothed[data_srs_smoothed.columns[:16]]\n",
    "#     del data_srs_no_null\n",
    "\n",
    "    gb = data_srs_smoothed.groupby(by=['Station'])\n",
    "    j=0\n",
    "    for i, item in gb:\n",
    "    #     print(item['Avg Speed'].rolling(5).mean())\n",
    "    #     station = item.Station[0]\n",
    "        data_srs_smoothed.loc[item.index, ('Avg Speed')] = item['Avg Speed'].rolling(5).mean()\n",
    "    #     j += 1\n",
    "    #     if j==2:\n",
    "    #         break\n",
    "    data_srs_smoothed.dropna(subset=['Avg Speed'], inplace=True)\n",
    "\n",
    "#     del srs_for_smoothing_gb\n",
    "\n",
    "    gb = data_srs_smoothed.groupby(by=['Station'])\n",
    "    non_sensitive_sts = gb.filter(lambda x: (x['Avg Speed'].std()<1))['Station'].unique()\n",
    "    data_srs_smoothed = data_srs_smoothed[~data_srs_smoothed.Station.isin(non_sensitive_sts)]\n",
    "    data_srs_smoothed.to_csv(dest_path+'/smoothed/'+child.name[:-4]+'.csv', index=False)\n",
    "\n",
    "    data_srs_smoothed = None\n",
    "\n",
    "    st_blacklist = np.append(st_blacklist, non_sensitive_sts[~np.isin(non_sensitive_sts, st_blacklist)])\n",
    "    gb, non_sensitive_sts = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(dest_path+'/stations_blacklist_dec.csv', st_blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/PeMS/Incidents/test_env/series/d07_text_station_5min_2017_03_01.tx.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_srs(src_pth):\n",
    "    pth = Path(src_pth + '/light')\n",
    "    st_blacklist = np.array([])\n",
    "    for child in pth.iterdir():\n",
    "        # incidents data reading\n",
    "        srs_file_name = child.name\n",
    "        print(srs_file_name)\n",
    "\n",
    "        data_srs_smoothed = pd.read_csv(src_pth + '/light/' + srs_file_name, sep=',', parse_dates=[0])\n",
    "\n",
    "        data_srs_smoothed.dropna(subset=['Avg Speed', 'Avg Occupancy', 'Total Flow'], inplace=True)\n",
    "\n",
    "        gb = data_srs_smoothed.groupby(by=['Station'])\n",
    "        j=0\n",
    "        for i, item in gb:\n",
    "\n",
    "            data_srs_smoothed.loc[item.index, ('Avg Speed')] = item['Avg Speed'].rolling(5).mean()\n",
    "            data_srs_smoothed.loc[item.index, ('Avg Occupancy')] = item['Avg Occupancy'].rolling(5).mean()\n",
    "            data_srs_smoothed.loc[item.index, ('Total Flow')] = item['Total Flow'].rolling(5).mean()\n",
    "\n",
    "        data_srs_smoothed.dropna(subset=['Avg Speed', 'Avg Occupancy', 'Total Flow'], inplace=True)\n",
    "\n",
    "\n",
    "        gb = data_srs_smoothed.groupby(by=['Station'])\n",
    "        non_sensitive_sts = gb.filter(lambda x: (x['Avg Speed'].std()<1))['Station'].unique()\n",
    "#         data_srs_smoothed = data_srs_smoothed[~data_srs_smoothed.Station.isin(non_sensitive_sts)]\n",
    "        data_srs_smoothed.to_csv(src_pth + '/smoothed/' + child.name[:-4]+'.csv', index=False)\n",
    "\n",
    "        st_blacklist = np.append(st_blacklist, non_sensitive_sts[~np.isin(non_sensitive_sts, st_blacklist)])\n",
    "    np.savetxt(src_pth + '/st_blacklist/stations_blacklist.csv', st_blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d07_text_station_5min_2017_01_08.tx.csv\n",
      "d07_text_station_5min_2017_01_20.tx.csv\n",
      "d07_text_station_5min_2017_01_04.tx.csv\n",
      "d07_text_station_5min_2017_01_02.tx.csv\n",
      "d07_text_station_5min_2017_01_05.tx.csv\n",
      "d07_text_station_5min_2017_01_06.tx.csv\n",
      "d07_text_station_5min_2017_01_21.tx.csv\n",
      "d07_text_station_5min_2017_01_11.tx.csv\n",
      "d07_text_station_5min_2017_01_18.tx.csv\n",
      "d07_text_station_5min_2017_01_09.tx.csv\n",
      "d07_text_station_5min_2017_01_29.tx.csv\n",
      "d07_text_station_5min_2017_01_13.tx.csv\n",
      "d07_text_station_5min_2017_01_23.tx.csv\n",
      "d07_text_station_5min_2017_01_19.tx.csv\n",
      "d07_text_station_5min_2017_01_24.tx.csv\n",
      "d07_text_station_5min_2017_01_03.tx.csv\n",
      "d07_text_station_5min_2017_01_07.tx.csv\n",
      "d07_text_station_5min_2017_01_25.tx.csv\n",
      "d07_text_station_5min_2017_01_26.tx.csv\n",
      "d07_text_station_5min_2017_01_14.tx.csv\n",
      "d07_text_station_5min_2017_01_12.tx.csv\n",
      "d07_text_station_5min_2017_01_01.tx.csv\n",
      "d07_text_station_5min_2017_01_15.tx.csv\n",
      "d07_text_station_5min_2017_01_16.tx.csv\n",
      "d07_text_station_5min_2017_01_30.tx.csv\n",
      "d07_text_station_5min_2017_01_22.tx.csv\n",
      "d07_text_station_5min_2017_01_17.tx.csv\n",
      "d07_text_station_5min_2017_01_10.tx.csv\n",
      "d07_text_station_5min_2017_01_31.tx.csv\n",
      "d07_text_station_5min_2017_01_28.tx.csv\n",
      "d07_text_station_5min_2017_01_27.tx.csv\n"
     ]
    }
   ],
   "source": [
    "smooth_srs('../data/PeMS/Incidents/work_folder/Months/Jan/series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d07_text_station_5min_2017_06_05.tx.csv\n",
      "d07_text_station_5min_2017_06_02.tx.csv\n",
      "d07_text_station_5min_2017_06_29.tx.csv\n",
      "d07_text_station_5min_2017_06_10.tx.csv\n",
      "d07_text_station_5min_2017_06_17.tx.csv\n",
      "d07_text_station_5min_2017_06_20.tx.csv\n",
      "d07_text_station_5min_2017_06_24.tx.csv\n",
      "d07_text_station_5min_2017_06_09.tx.csv\n",
      "d07_text_station_5min_2017_06_21.tx.csv\n",
      "d07_text_station_5min_2017_06_22.tx.csv\n",
      "d07_text_station_5min_2017_06_28.tx.csv\n",
      "d07_text_station_5min_2017_06_23.tx.csv\n",
      "d07_text_station_5min_2017_06_06.tx.csv\n",
      "d07_text_station_5min_2017_06_19.tx.csv\n",
      "d07_text_station_5min_2017_06_08.tx.csv\n",
      "d07_text_station_5min_2017_06_11.tx.csv\n",
      "d07_text_station_5min_2017_06_18.tx.csv\n",
      "d07_text_station_5min_2017_06_15.tx.csv\n",
      "d07_text_station_5min_2017_06_13.tx.csv\n",
      "d07_text_station_5min_2017_06_30.tx.csv\n",
      "d07_text_station_5min_2017_06_03.tx.csv\n",
      "d07_text_station_5min_2017_06_12.tx.csv\n",
      "d07_text_station_5min_2017_06_26.tx.csv\n",
      "d07_text_station_5min_2017_06_27.tx.csv\n",
      "d07_text_station_5min_2017_06_25.tx.csv\n",
      "d07_text_station_5min_2017_06_14.tx.csv\n",
      "d07_text_station_5min_2017_06_07.tx.csv\n",
      "d07_text_station_5min_2017_06_01.tx.csv\n",
      "d07_text_station_5min_2017_06_16.tx.csv\n",
      "d07_text_station_5min_2017_06_04.tx.csv\n",
      "d07_text_station_5min_2017_07_19.tx.csv\n",
      "d07_text_station_5min_2017_07_08.tx.csv\n",
      "d07_text_station_5min_2017_07_04.tx.csv\n",
      "d07_text_station_5min_2017_07_23.tx.csv\n",
      "d07_text_station_5min_2017_07_26.tx.csv\n",
      "d07_text_station_5min_2017_07_15.tx.csv\n",
      "d07_text_station_5min_2017_07_07.tx.csv\n",
      "d07_text_station_5min_2017_07_27.tx.csv\n",
      "d07_text_station_5min_2017_07_14.tx.csv\n",
      "d07_text_station_5min_2017_07_21.tx.csv\n",
      "d07_text_station_5min_2017_07_09.tx.csv\n",
      "d07_text_station_5min_2017_07_05.tx.csv\n",
      "d07_text_station_5min_2017_07_20.tx.csv\n",
      "d07_text_station_5min_2017_07_01.tx.csv\n",
      "d07_text_station_5min_2017_07_11.tx.csv\n",
      "d07_text_station_5min_2017_07_06.tx.csv\n",
      "d07_text_station_5min_2017_07_10.tx.csv\n",
      "d07_text_station_5min_2017_07_30.tx.csv\n",
      "d07_text_station_5min_2017_07_12.tx.csv\n",
      "d07_text_station_5min_2017_07_22.tx.csv\n",
      "d07_text_station_5min_2017_07_29.tx.csv\n",
      "d07_text_station_5min_2017_07_24.tx.csv\n",
      "d07_text_station_5min_2017_07_16.tx.csv\n",
      "d07_text_station_5min_2017_07_17.tx.csv\n",
      "d07_text_station_5min_2017_07_25.tx.csv\n",
      "d07_text_station_5min_2017_07_18.tx.csv\n",
      "d07_text_station_5min_2017_07_02.tx.csv\n",
      "d07_text_station_5min_2017_07_13.tx.csv\n",
      "d07_text_station_5min_2017_07_28.tx.csv\n",
      "d07_text_station_5min_2017_07_31.tx.csv\n",
      "d07_text_station_5min_2017_07_03.tx.csv\n",
      "d07_text_station_5min_2017_08_01.tx.csv\n",
      "d07_text_station_5min_2017_08_21.tx.csv\n",
      "d07_text_station_5min_2017_08_24.tx.csv\n",
      "d07_text_station_5min_2017_08_23.tx.csv\n",
      "d07_text_station_5min_2017_08_10.tx.csv\n",
      "d07_text_station_5min_2017_08_30.tx.csv\n",
      "d07_text_station_5min_2017_08_05.tx.csv\n",
      "d07_text_station_5min_2017_08_02.tx.csv\n",
      "d07_text_station_5min_2017_08_11.tx.csv\n",
      "d07_text_station_5min_2017_08_18.tx.csv\n",
      "d07_text_station_5min_2017_08_28.tx.csv\n",
      "d07_text_station_5min_2017_08_20.tx.csv\n",
      "d07_text_station_5min_2017_08_09.tx.csv\n",
      "d07_text_station_5min_2017_08_27.tx.csv\n",
      "d07_text_station_5min_2017_08_25.tx.csv\n",
      "d07_text_station_5min_2017_08_17.tx.csv\n",
      "d07_text_station_5min_2017_08_03.tx.csv\n",
      "d07_text_station_5min_2017_08_12.tx.csv\n",
      "d07_text_station_5min_2017_08_07.tx.csv\n",
      "d07_text_station_5min_2017_08_06.tx.csv\n",
      "d07_text_station_5min_2017_08_13.tx.csv\n",
      "d07_text_station_5min_2017_08_14.tx.csv\n",
      "d07_text_station_5min_2017_08_26.tx.csv\n",
      "d07_text_station_5min_2017_08_04.tx.csv\n",
      "d07_text_station_5min_2017_08_08.tx.csv\n",
      "d07_text_station_5min_2017_08_22.tx.csv\n",
      "d07_text_station_5min_2017_08_31.tx.csv\n",
      "d07_text_station_5min_2017_08_19.tx.csv\n",
      "d07_text_station_5min_2017_08_16.tx.csv\n",
      "d07_text_station_5min_2017_08_29.tx.csv\n",
      "d07_text_station_5min_2017_08_15.tx.csv\n",
      "d07_text_station_5min_2017_09_12.tx.csv\n",
      "d07_text_station_5min_2017_09_07.tx.csv\n",
      "d07_text_station_5min_2017_09_17.tx.csv\n",
      "d07_text_station_5min_2017_09_11.tx.csv\n",
      "d07_text_station_5min_2017_09_29.tx.csv\n",
      "d07_text_station_5min_2017_09_30.tx.csv\n",
      "d07_text_station_5min_2017_09_13.tx.csv\n",
      "d07_text_station_5min_2017_09_04.tx.csv\n",
      "d07_text_station_5min_2017_09_08.tx.csv\n",
      "d07_text_station_5min_2017_09_21.tx.csv\n",
      "d07_text_station_5min_2017_09_05.tx.csv\n",
      "d07_text_station_5min_2017_09_15.tx.csv\n",
      "d07_text_station_5min_2017_09_19.tx.csv\n",
      "d07_text_station_5min_2017_09_25.tx.csv\n",
      "d07_text_station_5min_2017_09_20.tx.csv\n",
      "d07_text_station_5min_2017_09_01.tx.csv\n",
      "d07_text_station_5min_2017_09_24.tx.csv\n",
      "d07_text_station_5min_2017_09_06.tx.csv\n",
      "d07_text_station_5min_2017_09_22.tx.csv\n",
      "d07_text_station_5min_2017_09_09.tx.csv\n",
      "d07_text_station_5min_2017_09_27.tx.csv\n",
      "d07_text_station_5min_2017_09_18.tx.csv\n",
      "d07_text_station_5min_2017_09_10.tx.csv\n",
      "d07_text_station_5min_2017_09_03.tx.csv\n",
      "d07_text_station_5min_2017_09_02.tx.csv\n",
      "d07_text_station_5min_2017_09_28.tx.csv\n",
      "d07_text_station_5min_2017_09_14.tx.csv\n",
      "d07_text_station_5min_2017_09_16.tx.csv\n",
      "d07_text_station_5min_2017_09_26.tx.csv\n",
      "d07_text_station_5min_2017_09_23.tx.csv\n",
      "d07_text_station_5min_2017_10_05.tx.csv\n",
      "d07_text_station_5min_2017_10_23.tx.csv\n",
      "d07_text_station_5min_2017_10_08.tx.csv\n",
      "d07_text_station_5min_2017_10_03.tx.csv\n",
      "d07_text_station_5min_2017_10_25.tx.csv\n",
      "d07_text_station_5min_2017_10_14.tx.csv\n",
      "d07_text_station_5min_2017_10_13.tx.csv\n",
      "d07_text_station_5min_2017_10_30.tx.csv\n",
      "d07_text_station_5min_2017_10_01.tx.csv\n",
      "d07_text_station_5min_2017_10_24.tx.csv\n",
      "d07_text_station_5min_2017_10_07.tx.csv\n",
      "d07_text_station_5min_2017_10_10.tx.csv\n",
      "d07_text_station_5min_2017_10_02.tx.csv\n",
      "d07_text_station_5min_2017_10_31.tx.csv\n",
      "d07_text_station_5min_2017_10_27.tx.csv\n",
      "d07_text_station_5min_2017_10_18.tx.csv\n",
      "d07_text_station_5min_2017_10_22.tx.csv\n",
      "d07_text_station_5min_2017_10_26.tx.csv\n",
      "d07_text_station_5min_2017_10_29.tx.csv\n",
      "d07_text_station_5min_2017_10_16.tx.csv\n",
      "d07_text_station_5min_2017_10_11.tx.csv\n",
      "d07_text_station_5min_2017_10_12.tx.csv\n",
      "d07_text_station_5min_2017_10_09.tx.csv\n",
      "d07_text_station_5min_2017_10_15.tx.csv\n",
      "d07_text_station_5min_2017_10_19.tx.csv\n",
      "d07_text_station_5min_2017_10_17.tx.csv\n",
      "d07_text_station_5min_2017_10_20.tx.csv\n",
      "d07_text_station_5min_2017_10_21.tx.csv\n",
      "d07_text_station_5min_2017_10_28.tx.csv\n",
      "d07_text_station_5min_2017_10_04.tx.csv\n",
      "d07_text_station_5min_2017_10_06.tx.csv\n",
      "d07_text_station_5min_2017_11_05.tx.csv\n",
      "d07_text_station_5min_2017_11_16.tx.csv\n",
      "d07_text_station_5min_2017_11_25.tx.csv\n",
      "d07_text_station_5min_2017_11_21.tx.csv\n",
      "d07_text_station_5min_2017_11_13.tx.csv\n",
      "d07_text_station_5min_2017_11_10.tx.csv\n",
      "d07_text_station_5min_2017_11_02.tx.csv\n",
      "d07_text_station_5min_2017_11_30.tx.csv\n",
      "d07_text_station_5min_2017_11_04.tx.csv\n",
      "d07_text_station_5min_2017_11_29.tx.csv\n",
      "d07_text_station_5min_2017_11_17.tx.csv\n",
      "d07_text_station_5min_2017_11_24.tx.csv\n",
      "d07_text_station_5min_2017_11_19.tx.csv\n",
      "d07_text_station_5min_2017_11_18.tx.csv\n",
      "d07_text_station_5min_2017_11_08.tx.csv\n",
      "d07_text_station_5min_2017_11_22.tx.csv\n",
      "d07_text_station_5min_2017_11_20.tx.csv\n",
      "d07_text_station_5min_2017_11_03.tx.csv\n",
      "d07_text_station_5min_2017_11_23.tx.csv\n",
      "d07_text_station_5min_2017_11_09.tx.csv\n",
      "d07_text_station_5min_2017_11_28.tx.csv\n",
      "d07_text_station_5min_2017_11_01.tx.csv\n",
      "d07_text_station_5min_2017_11_12.tx.csv\n",
      "d07_text_station_5min_2017_11_15.tx.csv\n",
      "d07_text_station_5min_2017_11_14.tx.csv\n",
      "d07_text_station_5min_2017_11_27.tx.csv\n",
      "d07_text_station_5min_2017_11_11.tx.csv\n",
      "d07_text_station_5min_2017_11_06.tx.csv\n",
      "d07_text_station_5min_2017_11_26.tx.csv\n",
      "d07_text_station_5min_2017_11_07.tx.csv\n",
      "d07_text_station_5min_2017_12_26.tx.csv\n",
      "d07_text_station_5min_2017_12_29.tx.csv\n",
      "d07_text_station_5min_2017_12_24.tx.csv\n",
      "d07_text_station_5min_2017_12_12.tx.csv\n",
      "d07_text_station_5min_2017_12_09.tx.csv\n",
      "d07_text_station_5min_2017_12_31.tx.csv\n",
      "d07_text_station_5min_2017_12_07.tx.csv\n",
      "d07_text_station_5min_2017_12_25.tx.csv\n",
      "d07_text_station_5min_2017_12_27.tx.csv\n",
      "d07_text_station_5min_2017_12_01.tx.csv\n",
      "d07_text_station_5min_2017_12_02.tx.csv\n",
      "d07_text_station_5min_2017_12_20.tx.csv\n",
      "d07_text_station_5min_2017_12_15.tx.csv\n",
      "d07_text_station_5min_2017_12_14.tx.csv\n",
      "d07_text_station_5min_2017_12_04.tx.csv\n",
      "d07_text_station_5min_2017_12_16.tx.csv\n",
      "d07_text_station_5min_2017_12_19.tx.csv\n",
      "d07_text_station_5min_2017_12_22.tx.csv\n",
      "d07_text_station_5min_2017_12_13.tx.csv\n",
      "d07_text_station_5min_2017_12_06.tx.csv\n",
      "d07_text_station_5min_2017_12_30.tx.csv\n",
      "d07_text_station_5min_2017_12_10.tx.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d07_text_station_5min_2017_12_08.tx.csv\n",
      "d07_text_station_5min_2017_12_21.tx.csv\n",
      "d07_text_station_5min_2017_12_23.tx.csv\n",
      "d07_text_station_5min_2017_12_03.tx.csv\n",
      "d07_text_station_5min_2017_12_28.tx.csv\n",
      "d07_text_station_5min_2017_12_05.tx.csv\n",
      "d07_text_station_5min_2017_12_17.tx.csv\n",
      "d07_text_station_5min_2017_12_18.tx.csv\n",
      "d07_text_station_5min_2017_12_11.tx.csv\n"
     ]
    }
   ],
   "source": [
    "dir_list = [\n",
    "            #'../data/PeMS/Incidents/work_folder/Months/Feb/series',\n",
    "#             '../data/PeMS/Incidents/work_folder/Months/Mar/series',\n",
    "#             '../data/PeMS/Incidents/work_folder/Months/Apr/series',\n",
    "#            '../data/PeMS/Incidents/work_folder/Months/May/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jun/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jul/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Aug/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Sep/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Oct/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Nov/series',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Dec/series']\n",
    "for a_dir in dir_list:\n",
    "    smooth_srs(a_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# traffic flow data reading\n",
    "header_srs = ['Timestamp', 'Station','District', 'Freeway', 'Direction of Travel', 'Lane Type', 'Station Length',\n",
    "          'Samples', '% Observed', 'Total Flow', 'Avg Occupancy', 'Avg Speed', \n",
    "          'Lane 1 Samples', 'Lane 1 Flow', 'Lane 1 Avg Occ', 'Lane 1 Avg Speed', 'Lane 1 Observed',\n",
    "          'Lane 2 Samples', 'Lane 2 Flow', 'Lane 2 Avg Occ', 'Lane 2 Avg Speed', 'Lane 2 Observed',\n",
    "          'Lane 3 Samples', 'Lane 3 Flow', 'Lane 3 Avg Occ', 'Lane 3 Avg Speed', 'Lane 3 Observed',\n",
    "          'Lane 4 Samples', 'Lane 4 Flow', 'Lane 4 Avg Occ', 'Lane 4 Avg Speed', 'Lane 4 Observed',\n",
    "          'Lane 5 Samples', 'Lane 5 Flow', 'Lane 5 Avg Occ', 'Lane 5 Avg Speed', 'Lane 5 Observed',\n",
    "          'Lane 6 Samples', 'Lane 6 Flow', 'Lane 6 Avg Occ', 'Lane 6 Avg Speed', 'Lane 6 Observed',\n",
    "          'Lane 7 Samples', 'Lane 7 Flow', 'Lane 7 Avg Occ', 'Lane 7 Avg Speed', 'Lane 7 Observed',\n",
    "          'Lane 8 Samples', 'Lane 8 Flow', 'Lane 8 Avg Occ', 'Lane 8 Avg Speed', 'Lane 8 Observed']\n",
    "\n",
    "srs_file_name = '../data/PeMS/Series/d07_text_station_5min_2017_10_11.txt'\n",
    "data_srs_no_null = pd.read_csv(srs_file_name, sep=',', names=header_srs, parse_dates=[0])\n",
    "data_srs_no_null = data_srs_no_null[data_srs_no_null.columns[:-40]]\n",
    "data_srs_no_null = data_srs_no_null.dropna(subset=['Avg Speed']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_srs_no_null.to_csv('../data/PeMS/Series/light/d07_text_station_5min_2017_10_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_srs_smoothed = data_srs_no_null[data_srs_no_null.columns[:16]].copy()\n",
    "srs_for_smoothing_gb = data_srs_smoothed.groupby(by=['Station'])\n",
    "j=0\n",
    "for i, item in srs_for_smoothing_gb:\n",
    "#     print(item['Avg Speed'].rolling(5).mean())\n",
    "#     station = item.Station[0]\n",
    "    data_srs_smoothed.loc[item.index, ('Avg Speed')] = item['Avg Speed'].rolling(5).mean()\n",
    "#     j += 1\n",
    "#     if j==2:\n",
    "#         break\n",
    "data_srs_smoothed = data_srs_smoothed.dropna(subset=['Avg Speed'])\n",
    "del srs_for_smoothing_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_srs_smoothed.to_csv('../data/PeMS/Series/smoothed/d07_text_station_5min_2017_10_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resave_stations(path, dest_path):\n",
    "    pth = Path(path)\n",
    "    header_stations = ['ID', 'Freeway', 'Freeway dir', 'Country', 'City', 'State Postmile', 'Abs Postmile', \n",
    "          'Latitude', 'Longitude', 'Length', 'Type', 'Lanes', 'Name',\n",
    "          'User ID1', 'User ID2', 'User ID3', 'User ID4']\n",
    "    for child in pth.iterdir():\n",
    "        st_data_file_name = path + '/' + child.name\n",
    "        stations_active = pd.read_csv(st_data_file_name, sep='\\t')#, names=header_stations)\n",
    "        stations_active = stations_active.astype({'Latitude':float, 'Longitude':float})\n",
    "        stations_active = stations_active.dropna(subset=['Latitude', 'Longitude']).reset_index(drop=True)\n",
    "        stations_active = stations_active.sort_values('Lanes', ascending=False).drop_duplicates(subset=['Latitude', 'Longitude'], keep='first').sort_index().reset_index(drop=True)\n",
    "        stations_active.to_csv(dest_path + '/' + child.name[:-4] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resave_stations('../data/PeMS/Stations/raw/', '../data/PeMS/Stations/light/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_stations = '../data/PeMS/Stations/d07_text_meta_2017_09_20.txt'\n",
    "header_stations = ['ID', 'Freeway', 'Freeway dir', 'Country', 'City', 'State Postmile', 'Abs Postmile', \n",
    "          'Latitude', 'Longitude', 'Length', 'Type', 'Lanes', 'Name',\n",
    "          'User ID1', 'User ID2', 'User ID3', 'User ID4']\n",
    "stations_active = pd.read_csv(file_stations, sep='\\t')\n",
    "stations_active = stations_active.astype({'Latitude':float, 'Longitude':float})\n",
    "stations_active = stations_active.dropna(subset=['Latitude', 'Longitude']).reset_index(drop=True)\n",
    "# result: non-empty speed data series  only, stations which provide these data\n",
    "stations_active = stations_active[stations_active['ID'].isin(data_srs_no_null['Station'].unique())].reset_index(drop=True)\n",
    "stations_active = stations_active.sort_values('Lanes', ascending=False).drop_duplicates(subset=['Latitude', 'Longitude'], keep='first').sort_index().reset_index(drop=True)\n",
    "stations_active = stations_active[~stations_active.ID.isin(st_blacklist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations_active.to_csv('../data/PeMS/Stations/light/d07_text_meta_2017_09_20_active.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# incidents week before data reading\n",
    "incdnt_file_name = '../data/PeMS/Incidents/all_text_chp_incident_day_2017_10_04.txt'\n",
    "data_inc_04_d07 = pd.read_csv(incdnt_file_name, sep=',', names=inc_header, parse_dates=[3])\n",
    "data_inc_04_d07 = (data_inc_04_d07.dropna(subset=['District']))[data_inc_04_d07.columns[:-4]]\n",
    "data_inc_04_d07 = data_inc_04_d07.astype(dtype={'District':int})\n",
    "# result: incidents in district under analysis\n",
    "data_inc_04_d07 = data_inc_04_d07[data_inc_04_d07['District']==7].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_inc_04_d07.to_csv('../data/PeMS/Incidents/light/all_text_chp_incident_day_2017_10_04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# traffic week b4 flow data reading\n",
    "srs_file_name = '../data/PeMS/Series/d07_text_station_5min_2017_10_04.txt'\n",
    "data_srs_no_null_04 = pd.read_csv(srs_file_name, sep=',', names=header_srs, parse_dates=[0])\n",
    "data_srs_no_null_04 = data_srs_no_null_04[data_srs_no_null_04.columns[:-40]]\n",
    "# result: non-empty speed data series  only, stations which provide these data\n",
    "data_srs_no_null_04 = data_srs_no_null_04.dropna(subset=['Avg Speed']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_srs_no_null_04.to_csv('../data/PeMS/Series/light/d07_text_station_5min_2017_10_04.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
