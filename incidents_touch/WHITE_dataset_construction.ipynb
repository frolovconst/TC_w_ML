{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import re\n",
    "from geopy.distance import vincenty\n",
    "import matplotlib.dates as mdts\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering of line blocking incidents only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_blk_desc(path):\n",
    "    dest_path = path + '/inc/blkg'\n",
    "    bound_re = re.compile(r\"(IF [E,N,S,W]B)\")\n",
    "    m_path = path + '/det'\n",
    "    pth = Path(m_path)\n",
    "    for child in pth.iterdir():\n",
    "        incdnt_det_file_name = m_path + '/' + child.name\n",
    "        det_month = pd.read_csv(incdnt_det_file_name, parse_dates=['Timestamp'])\n",
    "        blkg_desc = det_month[det_month.apply(lambda row: ('NOT BLK' not in row.Desc) & ((' BLKG' in row.Desc) | (' BLKD' in row.Desc) | (' BLKING' in row.Desc) | (' BLKG' in row.Desc)) & ((' ENTIR' in row.Desc) | (' LN' in row.Desc)) & (('IF LN' not in row.Desc)) & (not bound_re.search(row.Desc)), axis=1)]\n",
    "        blkg_desc.drop_duplicates(subset=['ID'], keep='first', inplace=True)\n",
    "        blkg_desc.to_csv(dest_path + '/' + child.name[:-4] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_list = [\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Jan/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Feb/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Mar/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Apr/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/May/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jun/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jul/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Aug/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Sep/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Oct/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Nov/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Dec/']\n",
    "for a_dir in dir_list:\n",
    "    filter_blk_desc(a_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of time windows with incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_downstream_station(stations, incident, correction=0):\n",
    "    fwy_no = incident.Freeway\n",
    "    fwy_dir = incident.Freeway_direction\n",
    "    lon = incident.Longitude\n",
    "    lat = incident.Latitude\n",
    "    candidates = stations[(stations['Fwy']==fwy_no) & (stations['Dir']==fwy_dir)].copy()\n",
    "    if fwy_dir == 'S':\n",
    "        candidates['Distance'] = lat - candidates['Latitude']\n",
    "        candidates['Distance_aux'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Latitude'] < lat - correction\n",
    "    elif fwy_dir == 'N':\n",
    "        candidates['Distance'] = candidates['Latitude'] - lat\n",
    "        candidates['Distance_aux'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Latitude'] > lat + correction\n",
    "    elif fwy_dir == 'W':\n",
    "        candidates['Distance'] = lon - candidates['Longitude']\n",
    "        candidates['Distance_aux'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Longitude'] < lon - correction\n",
    "    else : # fwy_dir == 'E':\n",
    "        candidates['Distance'] = candidates['Longitude'] - lon\n",
    "        candidates['Distance_aux'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Longitude'] > lon + correction\n",
    "    cond2 = (np.abs(candidates['Distance_aux'])<0.01) & (candidates['Distance']<0.01) & (candidates['Distance']>0)\n",
    "#     print(incidents[incidents['IncidentID']==incidentID].iloc[0])\n",
    "    return candidates[cond1 & cond2 & (candidates['Distance'] > 0)].sort_values(['Distance']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_upstream_station(stations, incident, correction=0):\n",
    "    fwy_no = incident.Freeway\n",
    "    fwy_dir = incident.Freeway_direction\n",
    "    lon = incident.Longitude\n",
    "    lat = incident.Latitude\n",
    "    candidates = stations[(stations['Fwy']==fwy_no) & (stations['Dir']==fwy_dir)].copy()\n",
    "    if fwy_dir == 'N':\n",
    "        candidates['Distance'] = lat - candidates['Latitude']\n",
    "        candidates['Distance_aux'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Latitude'] < lat - correction\n",
    "    elif fwy_dir == 'S':\n",
    "        candidates['Distance'] = candidates['Latitude'] - lat\n",
    "        candidates['Distance_aux'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Latitude'] > lat + correction\n",
    "    elif fwy_dir == 'E':\n",
    "        candidates['Distance'] = lon - candidates['Longitude']\n",
    "        candidates['Distance_aux'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Longitude'] < lon - correction\n",
    "    else : # fwy_dir == 'E':\n",
    "        candidates['Distance'] = candidates['Longitude'] - lon\n",
    "        candidates['Distance_aux'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Longitude'] > lon + correction\n",
    "    cond2 = (np.abs(candidates['Distance_aux'])<0.01) & (candidates['Distance']<0.01) & (candidates['Distance']>0)\n",
    "#     print(incidents[incidents['IncidentID']==incidentID].iloc[0])\n",
    "    return candidates[cond1 & cond2 & (candidates['Distance'] > 0)].sort_values(['Distance']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_wrapper(inc, func, stations):\n",
    "    nxt = func(stations, inc)\n",
    "    return nxt.iloc[0][['ID', 'Latitude', 'Longitude', 'Lanes']] if nxt.size>0 else pd.DataFrame(np.zeros((1,4), dtype=int), columns=['ID', 'Latitude', 'Longitude', 'Lanes']).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_time_series(an_inc, full_series, mins_before, mins_after, chrcteristic='Avg Occupancy'):\n",
    "    st = an_inc.ID_Prev\n",
    "    st_next = an_inc.ID_Next\n",
    "    test = full_series[(full_series.Station==st) &  (full_series.Timestamp >= an_inc.Timestamp-dt.timedelta(minutes=mins_before)) & (full_series.Timestamp < an_inc.Timestamp+dt.timedelta(minutes=mins_after))].copy()\n",
    "    test_next = full_series[(full_series.Station==st_next) &  (full_series.Timestamp >= an_inc.Timestamp-dt.timedelta(minutes=mins_before)) & (full_series.Timestamp < an_inc.Timestamp+dt.timedelta(minutes=mins_after))].copy()\n",
    "    return pd.concat((test[chrcteristic].reset_index(drop=True).add_prefix(chrcteristic + '_'),\n",
    "                      test_next[chrcteristic].reset_index(drop=True).add_prefix('Next ' + chrcteristic + '_')))\n",
    "#     test = test.reset_index(drop=True).reset_index()\n",
    "#     return test.pivot(index='Station', columns='index', values='Avg Occupancy').reset_index().drop(columns=[\"Station\"]).loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_accident_windows(path, mins_before=40, mins_after=40):\n",
    "    path_inc = path + '/inc/light'\n",
    "    pth = Path(path_inc)\n",
    "    srs_path = path + '/series/smoothed'\n",
    "    s_pth = Path(srs_path)\n",
    "    cols = ['IncidentID', 'Timestamp', 'Latitude', 'Longitude', 'Freeway', 'Freeway_direction']\n",
    "    result = pd.DataFrame()\n",
    "    stations_pth = path + '/stations'\n",
    "    st_pth = Path(stations_pth)\n",
    "    blk_path = path + '/inc/blkg'\n",
    "    blkg_desc = pd.read_csv(blk_path + '/' + sorted(Path(blk_path).iterdir())[0].name, parse_dates=['Timestamp'])\n",
    "    dest_path = path + '/result'\n",
    "    \n",
    "    for i,child in enumerate(sorted(pth.iterdir())):\n",
    "        incdnt_file_name = path_inc + '/' + child.name\n",
    "        inc_file = pd.read_csv(incdnt_file_name, parse_dates=['Timestamp'], dtype={'IncidentID':np.int64,'Latitude':np.float64,'Longitude':np.float64,'Freeway':np.int32, 'Freeway_direction':object})[cols]\n",
    "        inc_file.drop(index=inc_file[~inc_file.IncidentID.isin(blkg_desc.ID)].index, inplace=True)\n",
    "        day_start_time = dt.datetime.combine(inc_file.Timestamp.iloc[int(inc_file.shape[0]/2)].date(), dt.time(hour=0, minute=0))\n",
    "        min_timestamp = day_start_time + dt.timedelta(minutes=20+mins_before)\n",
    "        max_timestamp = day_start_time + dt.timedelta(hours=24, minutes=-mins_after)\n",
    "        inc_file.drop(index=inc_file[(inc_file.Timestamp<min_timestamp) | (inc_file.Timestamp>=max_timestamp)].index, inplace=True)\n",
    "        srs_f_name = srs_path + '/' + sorted(s_pth.iterdir())[i].name\n",
    "        srs = pd.read_csv(srs_f_name, parse_dates=['Timestamp'])\n",
    "        gb = srs.groupby('Station')\n",
    "        srs.drop(index=srs[srs.Station.isin(gb.filter(lambda x: len(x)<284).Station.unique())].index, inplace=True)\n",
    "        if srs.size == 0:\n",
    "            continue\n",
    "        st_active_file_name = stations_pth + '/' + sorted(st_pth.iterdir())[0].name\n",
    "        st_active = pd.read_csv(st_active_file_name).drop(columns=['User_ID_1', 'User_ID_2', 'User_ID_3', 'User_ID_4'])\n",
    "        st_active.drop(index=st_active[~st_active.ID.isin(srs.Station.unique())].index, inplace=True)\n",
    "        srs = srs.merge(st_active[['ID', 'Lanes']], how='left', left_on='Station', right_on='ID')\n",
    "        srs['Total Flow'] = srs['Total Flow']/srs['Lanes']\n",
    "        inc_file_prv = inc_file.apply(get_next_wrapper, axis=1, args=(get_next_upstream_station, st_active))\n",
    "        for col in inc_file_prv.columns:\n",
    "            inc_file[col+'_Prev'] = inc_file_prv[col]\n",
    "        inc_file_nxt = inc_file.apply(get_next_wrapper, axis=1, args=(get_next_downstream_station, st_active))\n",
    "        for col in inc_file_nxt.columns:\n",
    "            inc_file[col+'_Next'] = inc_file_nxt[col]\n",
    "        \n",
    "        inc_file.drop(index=inc_file[(inc_file.ID_Prev==0) | (inc_file.ID_Next==0)].index, inplace=True)\n",
    "        if inc_file.size==0:\n",
    "            continue\n",
    "        windows = inc_file.apply(obtain_time_series, axis=1, args=[srs, mins_before, mins_after, 'Avg Occupancy'])\n",
    "        inc_file = pd.concat((inc_file, windows), axis=1)\n",
    "        windows = inc_file.apply(obtain_time_series, axis=1, args=[srs, mins_before, mins_after, 'Total Flow'])\n",
    "        inc_file = pd.concat((inc_file, windows), axis=1)\n",
    "        windows = inc_file.apply(obtain_time_series, axis=1, args=[srs, mins_before, mins_after, 'Avg Speed'])\n",
    "        inc_file = pd.concat((inc_file, windows), axis=1)\n",
    "        result = pd.concat((result, inc_file), axis=0)\n",
    "#         break\n",
    "    result.to_csv(dest_path + '/accident_windows_next.csv', index=False)\n",
    "    print(path + ' complete')\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_list = [\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Jan/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Feb/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Mar/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Apr/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/May/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jun/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jul/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Aug/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Sep/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Oct/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Nov/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Dec/']\n",
    "for a_dir in dir_list:\n",
    "    accdnt_windows = create_accident_windows(a_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accdnt_windows = pd.DataFrame()\n",
    "dir_list = [\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Jan/result/accident_windows_next.csv',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Feb/result/accident_windows_next.csv',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Mar/result/accident_windows_next.csv',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Apr/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/May/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jun/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jul/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Aug/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Sep/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Oct/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Nov/result/accident_windows_next.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Dec/result/accident_windows_next.csv']\n",
    "for a_file in dir_list:\n",
    "    accdnt_windows = pd.concat((accdnt_windows, pd.read_csv(a_file, parse_dates=['Timestamp'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# col_order = accdnt_windows.columns[[26,30,27,28,24,25,29,0,1,4,5,6,7,8,9,10,11,2,3,31,32,35,36,37,38,39,40,41,42,33,34,12,13,16,17,18,19,20,21,22,23,14,15]]\n",
    "\n",
    "col_order = ['IncidentID', 'Timestamp', 'Latitude_Prev', 'Latitude_Next', 'Longitude_Prev', 'Longitude_Next', 'Freeway',\n",
    "       'Freeway_direction', 'ID_Prev', 'ID_Next',\n",
    "             'Lanes_Prev', 'Lanes_Next',\n",
    "        'Avg Occupancy_0',\n",
    "       'Avg Occupancy_1', 'Avg Occupancy_2', 'Avg Occupancy_3',\n",
    "       'Avg Occupancy_4', 'Avg Occupancy_5', 'Avg Occupancy_6',\n",
    "       'Avg Occupancy_7', 'Avg Occupancy_8', 'Avg Occupancy_9',\n",
    "       'Avg Occupancy_10', 'Avg Occupancy_11', 'Avg Occupancy_12',\n",
    "       'Avg Occupancy_13', 'Avg Occupancy_14', 'Avg Occupancy_15',\n",
    "       'Total Flow_0',\n",
    "       'Total Flow_1', 'Total Flow_2', 'Total Flow_3', 'Total Flow_4',\n",
    "       'Total Flow_5', 'Total Flow_6', 'Total Flow_7', 'Total Flow_8',\n",
    "       'Total Flow_9', 'Total Flow_10', 'Total Flow_11', 'Total Flow_12',\n",
    "       'Total Flow_13', 'Total Flow_14', 'Total Flow_15',\n",
    "       'Avg Speed_0', 'Avg Speed_1', 'Avg Speed_2',\n",
    "       'Avg Speed_3', 'Avg Speed_4', 'Avg Speed_5', 'Avg Speed_6',\n",
    "       'Avg Speed_7', 'Avg Speed_8', 'Avg Speed_9', 'Avg Speed_10',\n",
    "       'Avg Speed_11', 'Avg Speed_12', 'Avg Speed_13', 'Avg Speed_14',\n",
    "       'Avg Speed_15',\n",
    "        'Next Avg Occupancy_0', 'Next Avg Occupancy_1','Next Avg Occupancy_2',\n",
    "        'Next Avg Occupancy_3', 'Next Avg Occupancy_4', 'Next Avg Occupancy_5',\n",
    "        'Next Avg Occupancy_6', 'Next Avg Occupancy_7', 'Next Avg Occupancy_8',\n",
    "             'Next Avg Occupancy_9','Next Avg Occupancy_10','Next Avg Occupancy_11',\n",
    "             'Next Avg Occupancy_12','Next Avg Occupancy_13','Next Avg Occupancy_14',\n",
    "             'Next Avg Occupancy_15','Next Total Flow_0','Next Total Flow_1',\n",
    "             'Next Total Flow_2','Next Total Flow_3','Next Total Flow_4',\n",
    "             'Next Total Flow_5','Next Total Flow_6','Next Total Flow_7',\n",
    "             'Next Total Flow_8','Next Total Flow_9','Next Total Flow_10',\n",
    "             'Next Total Flow_11','Next Total Flow_12','Next Total Flow_13',\n",
    "             'Next Total Flow_14','Next Total Flow_15','Next Avg Speed_0',\n",
    "             'Next Avg Speed_1','Next Avg Speed_2','Next Avg Speed_3',\n",
    "             'Next Avg Speed_4','Next Avg Speed_5','Next Avg Speed_6',\n",
    "             'Next Avg Speed_7','Next Avg Speed_8','Next Avg Speed_9',\n",
    "             'Next Avg Speed_10','Next Avg Speed_11','Next Avg Speed_12',\n",
    "             'Next Avg Speed_13','Next Avg Speed_14','Next Avg Speed_15'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accdnt_windows = accdnt_windows[col_order]\n",
    "\n",
    "accdnt_windows.to_csv('../data/PeMS/Incidents/work_folder/year_accdnt_wndw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of incident-free windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_neighbour(stations, station, correction=0):\n",
    "    fwy_no = station.Fwy\n",
    "    fwy_dir = station.Dir\n",
    "    lon = station.Longitude\n",
    "    lat = station.Latitude\n",
    "    candidates = stations[(stations['Fwy']==fwy_no) & (stations['Dir']==fwy_dir)].copy()\n",
    "    if fwy_dir == 'S':\n",
    "        candidates['Distance'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Latitude'] < lat - correction\n",
    "    elif fwy_dir == 'N':\n",
    "        candidates['Distance'] = candidates['Latitude'] - lat\n",
    "        cond1 = candidates['Latitude'] > lat + correction\n",
    "    elif fwy_dir == 'W':\n",
    "        candidates['Distance'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Longitude'] < lon - correction\n",
    "    else : # fwy_dir == 'E':\n",
    "        candidates['Distance'] = candidates['Longitude'] - lon\n",
    "        cond1 = candidates['Longitude'] > lon + correction\n",
    "    cond2 = (np.abs(candidates['Distance'])<0.025)\n",
    "    candidates = candidates[cond1 & cond2 & (candidates['Distance'] > 0)].sort_values(['Distance']).reset_index(drop=True)\n",
    "    return candidates.iloc[0].ID if candidates.size>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_neighbour_full_data(stations, station, correction=0):\n",
    "    fwy_no = station.Fwy\n",
    "    fwy_dir = station.Dir\n",
    "    lon = station.Longitude\n",
    "    lat = station.Latitude\n",
    "    candidates = stations[(stations['Fwy']==fwy_no) & (stations['Dir']==fwy_dir)].copy()\n",
    "    if fwy_dir == 'S':\n",
    "        candidates['Distance'] = lat - candidates['Latitude']\n",
    "        candidates['Distance_aux'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Latitude'] < lat - correction\n",
    "    elif fwy_dir == 'N':\n",
    "        candidates['Distance'] = candidates['Latitude'] - lat\n",
    "        candidates['Distance_aux'] = lon - candidates['Longitude']\n",
    "        cond1 = candidates['Latitude'] > lat + correction\n",
    "    elif fwy_dir == 'W':\n",
    "        candidates['Distance'] = lon - candidates['Longitude']\n",
    "        candidates['Distance_aux'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Longitude'] < lon - correction\n",
    "    else : # fwy_dir == 'E':\n",
    "        candidates['Distance'] = candidates['Longitude'] - lon\n",
    "        candidates['Distance_aux'] = lat - candidates['Latitude']\n",
    "        cond1 = candidates['Longitude'] > lon + correction\n",
    "    cond2 = (candidates['Distance']<0.01) & (candidates['Distance']>0) & (np.abs(candidates['Distance_aux'])<.01)\n",
    "    candidates = candidates[cond1 & cond2 & (candidates['Distance'] > 0)].sort_values(['Distance']).reset_index(drop=True)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_inc_eligibility(an_inc, a_station, vicinity_km, timestamp, td):\n",
    "    st_loc = (a_station[['Latitude', 'Longitude']])\n",
    "    inc_tmsmp = an_inc.Timestamp\n",
    "    cnd1 = vincenty((an_inc.Latitude, an_inc.Longitude), st_loc).kilometers < vicinity_km\n",
    "    cnd2 = (inc_tmsmp>=timestamp-td) & (inc_tmsmp<timestamp+td)\n",
    "    return cnd1 & cnd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_inc_time(an_inc, timestamp, td_bf, td_aft):\n",
    "    inc_tmsmp = an_inc.Timestamp\n",
    "    cnd = (inc_tmsmp>=timestamp-td_bf) & (inc_tmsmp<timestamp+td_aft)\n",
    "    return cnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_inc_location(an_inc, a_station, vicinity_km):\n",
    "    st_loc = (a_station[['Latitude', 'Longitude']])\n",
    "    cnd = vincenty((an_inc.Latitude, an_inc.Longitude), st_loc).kilometers < vicinity_km\n",
    "    return cnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_time_series_clear(station, stations, full_series, incs, vicinity_km, mins_before, mins_after, chrcteristics=['Avg Occupancy', 'Total Flow']):#, 'Avg Speed']):\n",
    "#     np.random.seed(14)\n",
    "#     print(station)\n",
    "    TICK_SIZE = 5 # minutes\n",
    "    TICKS_COUNT = 284\n",
    "    ticks_before = int(mins_before/TICK_SIZE)\n",
    "    ticks_after = int(mins_after/TICK_SIZE)\n",
    "    wnd_centre_nmb = np.random.randint(TICKS_COUNT-ticks_before-ticks_after)+ticks_before\n",
    "    timestamp = full_series[full_series.Station==station.ID]['Timestamp'].iloc[wnd_centre_nmb] \n",
    "    timedelta_bf = dt.timedelta(minutes = mins_before)\n",
    "    timedelta_aft = dt.timedelta(minutes = mins_after)\n",
    "    incs_nearby = incs[incs.apply(check_inc_time, axis=1, args=(timestamp, timedelta_bf, timedelta_aft))]\n",
    "    if incs_nearby.size > 0:\n",
    "        incs_nearby = incs_nearby[incs_nearby.apply(check_inc_location, axis=1, args=(station, vicinity_km))]\n",
    "    \n",
    "    if incs_nearby.size == 0:\n",
    "        next_st = get_next_neighbour_full_data(stations, station)\n",
    "        if next_st.size == 0:\n",
    "            return pd.Series(None)\n",
    "        else:\n",
    "            next_st = next_st.iloc[0] \n",
    "        test = full_series[(full_series.Station==station.ID) & (full_series.Timestamp >= timestamp-timedelta_bf) & (full_series.Timestamp < timestamp+timedelta_aft)]\n",
    "        test_next = full_series[(full_series.Station==next_st.ID) & (full_series.Timestamp >= timestamp-timedelta_bf) & (full_series.Timestamp < timestamp+timedelta_aft)]\n",
    "        result=pd.Series()\n",
    "        result.at['Timestamp'] = timestamp\n",
    "        result.at['Latitude_Prev'] = station['Latitude']\n",
    "        result.at['Longitude_Prev'] = station['Longitude']\n",
    "        result.at['Latitude_Next'] = next_st['Latitude']\n",
    "        result.at['Longitude_Next'] = next_st['Longitude']\n",
    "        result.at['ID_Prev'] = station['ID']\n",
    "        result.at['ID_Next'] = next_st['ID']\n",
    "        result.at['Lanes_Prev'] = station['Lanes']\n",
    "        result.at['Lanes_Next'] = next_st['Lanes']\n",
    "        res = []\n",
    "        for chrct in chrcteristics:\n",
    "            res += [test[chrct].reset_index(drop=True).add_prefix(chrct + '_'),]\n",
    "            res += [test_next[chrct].reset_index(drop=True).add_prefix('Next ' + chrct + '_'),]\n",
    "        res = pd.concat(res, axis=0)\n",
    "        return result.append(res)\n",
    "    \n",
    "    return pd.Series(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_accident_free_windows(path, vicinity_km=3, mins_before=40, mins_after=40):\n",
    "    np.random.seed(14)\n",
    "    srs_path = path + '/series/smoothed'\n",
    "    pth = Path(srs_path)\n",
    "    inc_path = path + '/inc/light'\n",
    "    inc_pth = Path(inc_path)\n",
    "    st_path = path + '/stations'\n",
    "    st_pth = Path(st_path)\n",
    "    dest_path = path + '/result'\n",
    "    inc_cols = ['Timestamp', 'Latitude', 'Longitude', 'Freeway']\n",
    "    st_cols = ['Latitude', 'Longitude', 'ID', 'Lanes', 'Dir', 'Fwy']\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for i,child in enumerate(sorted(pth.iterdir())):\n",
    "        srs_file_name = srs_path + '/' + child.name\n",
    "        srs_file = pd.read_csv(srs_file_name, parse_dates=['Timestamp'])\n",
    "        gb = srs_file.groupby('Station')\n",
    "        srs_file.drop(index=srs_file[srs_file.Station.isin(gb.filter(lambda x: len(x)<284).Station.unique())].index, inplace=True)\n",
    "        if srs_file.size == 0:\n",
    "            continue\n",
    "        \n",
    "        inc_f_name = inc_path + '/' + sorted(inc_pth.iterdir())[i].name\n",
    "        incs = pd.read_csv(inc_f_name, parse_dates=['Timestamp'])[inc_cols]\n",
    "        st_active_file_name = st_path + '/' + sorted(st_pth.iterdir())[0].name\n",
    "        st_active = pd.read_csv(st_active_file_name)[st_cols]\n",
    "        st_active.drop(index=st_active[~st_active.ID.isin(srs_file.Station.unique())].index, inplace=True)\n",
    "        srs_file = srs_file.merge(st_active[['ID', 'Lanes']], how='left', left_on='Station', right_on='ID')\n",
    "        srs_file['Total Flow'] = srs_file['Total Flow']/srs_file['Lanes']\n",
    "        size_rdm = 120\n",
    "\n",
    "        \n",
    "        stations_rdm = st_active[st_active.ID.isin(np.random.choice(st_active.ID, size_rdm, replace=False))]\n",
    "        windows = stations_rdm.apply(obtain_time_series_clear, args=(st_active, srs_file, incs, vicinity_km, mins_before, mins_after, ['Avg Occupancy', 'Total Flow', 'Avg Speed']), axis=1)\n",
    "        windows.dropna(axis=0, how='any', inplace=True)\n",
    "        \n",
    "        \n",
    "        result = pd.concat((result, windows), axis=0)\n",
    "#         if i>1:\n",
    "#         break\n",
    "    result.to_csv(dest_path + '/accident_free_windows.csv', index=False)\n",
    "    print(path + ' complete')\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_list = [\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Jan/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Feb/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Mar/',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Apr/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/May/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jun/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jul/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Aug/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Sep/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Oct/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Nov/',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Dec/']\n",
    "for a_dir in dir_list:\n",
    "    accdnt_free_windows = create_accident_free_windows(a_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accdnt_free_windows = pd.DataFrame()\n",
    "dir_list = [\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Jan/result/accident_free_windows.csv',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Feb/result/accident_free_windows.csv',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Mar/result/accident_free_windows.csv',\n",
    "            '../data/PeMS/Incidents/work_folder/Months/Apr/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/May/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jun/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Jul/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Aug/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Sep/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Oct/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Nov/result/accident_free_windows.csv',\n",
    "           '../data/PeMS/Incidents/work_folder/Months/Dec/result/accident_free_windows.csv']\n",
    "for a_file in dir_list:\n",
    "    accdnt_free_windows = pd.concat((accdnt_free_windows, pd.read_csv(a_file, parse_dates=['Timestamp'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_order = ['ID_Prev', 'ID_Next', 'Timestamp', 'Latitude_Next', 'Longitude_Next', 'Latitude_Prev', 'Longitude_Prev',\n",
    "             'Lanes_Prev', 'Lanes_Next',\n",
    "             'Avg Occupancy_0',\n",
    "       'Avg Occupancy_1', 'Avg Occupancy_2', 'Avg Occupancy_3',\n",
    "       'Avg Occupancy_4', 'Avg Occupancy_5', 'Avg Occupancy_6',\n",
    "       'Avg Occupancy_7', 'Avg Occupancy_8', 'Avg Occupancy_9',\n",
    "       'Avg Occupancy_10', 'Avg Occupancy_11', 'Avg Occupancy_12',\n",
    "       'Avg Occupancy_13', 'Avg Occupancy_14', 'Avg Occupancy_15',\n",
    "       'Total Flow_0',\n",
    "       'Total Flow_1', 'Total Flow_2', 'Total Flow_3', 'Total Flow_4',\n",
    "       'Total Flow_5', 'Total Flow_6', 'Total Flow_7', 'Total Flow_8',\n",
    "       'Total Flow_9', 'Total Flow_10', 'Total Flow_11', 'Total Flow_12',\n",
    "       'Total Flow_13', 'Total Flow_14', 'Total Flow_15',\n",
    "       'Avg Speed_0', 'Avg Speed_1', 'Avg Speed_2',\n",
    "       'Avg Speed_3', 'Avg Speed_4', 'Avg Speed_5', 'Avg Speed_6',\n",
    "       'Avg Speed_7', 'Avg Speed_8', 'Avg Speed_9', 'Avg Speed_10',\n",
    "       'Avg Speed_11', 'Avg Speed_12', 'Avg Speed_13', 'Avg Speed_14',\n",
    "       'Avg Speed_15',\n",
    "        'Next Avg Occupancy_0', 'Next Avg Occupancy_1','Next Avg Occupancy_2',\n",
    "        'Next Avg Occupancy_3', 'Next Avg Occupancy_4', 'Next Avg Occupancy_5',\n",
    "        'Next Avg Occupancy_6', 'Next Avg Occupancy_7', 'Next Avg Occupancy_8',\n",
    "             'Next Avg Occupancy_9','Next Avg Occupancy_10','Next Avg Occupancy_11',\n",
    "             'Next Avg Occupancy_12','Next Avg Occupancy_13','Next Avg Occupancy_14',\n",
    "             'Next Avg Occupancy_15','Next Total Flow_0','Next Total Flow_1',\n",
    "             'Next Total Flow_2','Next Total Flow_3','Next Total Flow_4',\n",
    "             'Next Total Flow_5','Next Total Flow_6','Next Total Flow_7',\n",
    "             'Next Total Flow_8','Next Total Flow_9','Next Total Flow_10',\n",
    "             'Next Total Flow_11','Next Total Flow_12','Next Total Flow_13',\n",
    "             'Next Total Flow_14','Next Total Flow_15','Next Avg Speed_0',\n",
    "             'Next Avg Speed_1','Next Avg Speed_2','Next Avg Speed_3',\n",
    "             'Next Avg Speed_4','Next Avg Speed_5','Next Avg Speed_6',\n",
    "             'Next Avg Speed_7','Next Avg Speed_8','Next Avg Speed_9',\n",
    "             'Next Avg Speed_10','Next Avg Speed_11','Next Avg Speed_12',\n",
    "             'Next Avg Speed_13','Next Avg Speed_14','Next Avg Speed_15' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accdnt_free_windows = accdnt_free_windows[col_order]\n",
    "\n",
    "accdnt_free_windows.to_csv('../data/PeMS/Incidents/work_folder/year_accdnt_free_wndw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging of two window classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accdnt_windows = pd.read_csv('../data/PeMS/Incidents/work_folder/year_accdnt_wndw.csv', parse_dates=['Timestamp'])\n",
    "\n",
    "accdnt_windows['y'] = 1\n",
    "\n",
    "accdnt_free_windows = pd.read_csv('../data/PeMS/Incidents/work_folder/year_accdnt_free_wndw.csv', parse_dates=['Timestamp'])\n",
    "# accdnt_free_windows['IncidentID'] = 0\n",
    "\n",
    "accdnt_free_windows['y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_cols = ['ID_Prev', 'ID_Next', 'Timestamp', 'Latitude_Next', 'Longitude_Next', 'Latitude_Prev', 'Longitude_Prev',\n",
    "               'Lanes_Prev', 'Lanes_Next',\n",
    "             'Avg Occupancy_0',\n",
    "       'Avg Occupancy_1', 'Avg Occupancy_2', 'Avg Occupancy_3',\n",
    "       'Avg Occupancy_4', 'Avg Occupancy_5', 'Avg Occupancy_6',\n",
    "       'Avg Occupancy_7', 'Avg Occupancy_8', 'Avg Occupancy_9',\n",
    "       'Avg Occupancy_10', 'Avg Occupancy_11', 'Avg Occupancy_12',\n",
    "       'Avg Occupancy_13', 'Avg Occupancy_14', 'Avg Occupancy_15',\n",
    "       'Total Flow_0',\n",
    "       'Total Flow_1', 'Total Flow_2', 'Total Flow_3', 'Total Flow_4',\n",
    "       'Total Flow_5', 'Total Flow_6', 'Total Flow_7', 'Total Flow_8',\n",
    "       'Total Flow_9', 'Total Flow_10', 'Total Flow_11', 'Total Flow_12',\n",
    "       'Total Flow_13', 'Total Flow_14', 'Total Flow_15',\n",
    "       'Avg Speed_0', 'Avg Speed_1', 'Avg Speed_2',\n",
    "       'Avg Speed_3', 'Avg Speed_4', 'Avg Speed_5', 'Avg Speed_6',\n",
    "       'Avg Speed_7', 'Avg Speed_8', 'Avg Speed_9', 'Avg Speed_10',\n",
    "       'Avg Speed_11', 'Avg Speed_12', 'Avg Speed_13', 'Avg Speed_14',\n",
    "       'Avg Speed_15',\n",
    "        'Next Avg Occupancy_0', 'Next Avg Occupancy_1','Next Avg Occupancy_2',\n",
    "        'Next Avg Occupancy_3', 'Next Avg Occupancy_4', 'Next Avg Occupancy_5',\n",
    "        'Next Avg Occupancy_6', 'Next Avg Occupancy_7', 'Next Avg Occupancy_8',\n",
    "             'Next Avg Occupancy_9','Next Avg Occupancy_10','Next Avg Occupancy_11',\n",
    "             'Next Avg Occupancy_12','Next Avg Occupancy_13','Next Avg Occupancy_14',\n",
    "             'Next Avg Occupancy_15','Next Total Flow_0','Next Total Flow_1',\n",
    "             'Next Total Flow_2','Next Total Flow_3','Next Total Flow_4',\n",
    "             'Next Total Flow_5','Next Total Flow_6','Next Total Flow_7',\n",
    "             'Next Total Flow_8','Next Total Flow_9','Next Total Flow_10',\n",
    "             'Next Total Flow_11','Next Total Flow_12','Next Total Flow_13',\n",
    "             'Next Total Flow_14','Next Total Flow_15','Next Avg Speed_0',\n",
    "             'Next Avg Speed_1','Next Avg Speed_2','Next Avg Speed_3',\n",
    "             'Next Avg Speed_4','Next Avg Speed_5','Next Avg Speed_6',\n",
    "             'Next Avg Speed_7','Next Avg Speed_8','Next Avg Speed_9',\n",
    "             'Next Avg Speed_10','Next Avg Speed_11','Next Avg Speed_12',\n",
    "             'Next Avg Speed_13','Next Avg Speed_14','Next Avg Speed_15',\n",
    "              'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat((accdnt_windows[common_cols], accdnt_free_windows[common_cols]), axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['Hour'] = dataset.Timestamp.dt.hour\n",
    "\n",
    "dataset = dataset.drop(index=dataset[(dataset['Hour']<6) | (dataset['Hour']>21)].index).reset_index(drop=True)\n",
    "\n",
    "np.random.seed=442\n",
    "dataset = dataset.reindex(index=np.random.permutation(dataset.index))\n",
    "\n",
    "dataset['Width_change'] = dataset.apply(lambda x: 1 if x.Lanes_Prev==x.Lanes_Next else 0, axis=1)\n",
    "\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('../data/PeMS/Incidents/work_folder/windows.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
